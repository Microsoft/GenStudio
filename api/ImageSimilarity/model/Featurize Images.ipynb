{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize Images for Image Similarity Model\n",
    "\n",
    "This notebook will do the following:\n",
    "\n",
    "For each image in the provided Azure Blob container:\n",
    "    - download the image\n",
    "    - resize the image to the pre-defined img_width & img_height\n",
    "    - Featurize the image using the Keras pre-trained ResNet50 model trained on imagenet\n",
    "    - save the featurized images to a preprocessedimages.pkl file in the provide data directory\n",
    "    - save a corresponding targets.pkl file with a table of the [name, url] for each image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "from urllib.request import urlopen\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from azure.storage.blob import BlockBlobService, PublicAccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False #used to turn print statements on/off\n",
    "\n",
    "url_base = '' #Root folder url to where the images are stored in Azure Blob Storage\n",
    "blob_account_name = '' #Azure storage account name\n",
    "blob_sas_token='' # SAS token to access the blob\n",
    "blob_container = '' #container where the images are located\n",
    "blob_prefix = '' #path to any sub-folders within the container \n",
    "\n",
    "\n",
    "\n",
    "sample_length = 10 #number of images to pre-process\n",
    "print_every = 100 #print update every print_every iterations\n",
    "img_width = 512 #input sizes images will be re-szied to\n",
    "img_height = 512\n",
    "\n",
    "filename_root = '' #root folder filepath. All data from this notebookwill be saved to this directory\n",
    "pre_processed_filename = filename_root + 'preprocessedimages' #name of the np array of size (sample_length, img_length) will be saved. These are the featurized versions of the images\n",
    "targets_filename = filename_root + 'targets' # helper table that tracks the name & URL for each row\n",
    "failed_filename = filename_root + 'failed' # save and record any failures\n",
    "total_processed = filename_root + 'total_i.pkl' # save the total number\n",
    "\n",
    "\n",
    "#check that the all the variables have been set\n",
    "assert url_base != '', 'Please provide the root url for all the images. Example: if all images are at https://test.com/image1.jpg, provide https://test.com/'\n",
    "assert blob_account_name != '', 'Please provide the Azure storage account name where the images are stored'\n",
    "assert blob_sas_token != '', 'Please provide the SAS token for accessing the blob account'\n",
    "assert blob_container != '', 'Please provide the container name where the images are stored'\n",
    "assert blob_prefix !='', 'Please provide any additional path compnoents for the imates. Example if the iamges are stored in containername/data/images the prefix is data/images'\n",
    "assert filename_root != '', 'Please provide a filepath for where the data should be saved. Example: /data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Resnet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model\n",
    "img_length = 2048 #size of output from model\n",
    "keras_model = ResNet50(input_shape=[img_width,img_height,3], \n",
    "                     weights='imagenet', \n",
    "                     include_top=False, \n",
    "                     pooling='avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize images with ResNet50 Model & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "#Connect to the blob\n",
    "block_blob_service = BlockBlobService(account_name=blob_account_name, sas_token=blob_sas_token)\n",
    "files = block_blob_service.list_blobs(blob_container, prefix=blob_prefix)\n",
    "\n",
    "targets = []\n",
    "failed_idx = []\n",
    "preprocessed_images = np.zeros((sample_length,img_length))\n",
    "\n",
    "\n",
    "#Loop through all files in the blob container location provided. We will download each iamge & run it through the ResNet50 model.\n",
    "i = 0\n",
    "for file in files:\n",
    "    name = file.name.rsplit('/',1)[-1].split('.')[0]  #this is the object ID\n",
    "    url = url_base + name + '.jpg'\n",
    "    try:\n",
    "        #download image\n",
    "        with urlopen(url) as file:\n",
    "            img = Image.open(file)\n",
    "            #non RGB images won't have the right number of channels\n",
    "            if img.mode != 'RGB': \n",
    "                img = img.convert('RGB') \n",
    "                \n",
    "        #re-size, expand dims and run through the ResNet50 model\n",
    "        img = np.array(img.resize((img_width, img_height)))\n",
    "        img = preprocess_input(np.expand_dims(img, axis=0).astype(np.float))\n",
    "        img = keras_model.predict(img)\n",
    "        #add to master table\n",
    "        preprocessed_images[i,:] = img\n",
    "        targets.append([name, url])\n",
    "    except Exception as e:\n",
    "        print('failed to process: %s' % url)\n",
    "        print('iteration %d' % i)\n",
    "        print(e)\n",
    "        failed_idx.append(i)\n",
    "    \n",
    "    if i%print_every == 0:\n",
    "        print('completed iteration: %d' % i)\n",
    "        print('saving model to file ')\n",
    "        pickle.dump(preprocessed_images, open(pre_processed_filename + str(i) +'.pkl', 'wb'))\n",
    "        pickle.dump(targets,open(targets_filename + str(i) +'.pkl','wb'))\n",
    "        pickle.dump(failed_idx, open(failed_filename + str(i) + '.pkl','wb'))\n",
    "        current_time = time.time()\n",
    "        print('elapsed time %0.2f min' % ((current_time - start)/60))\n",
    "    \n",
    "    i += 1\n",
    "    if i>= sample_length:\n",
    "        print('Reached the end, breaking-loop')\n",
    "        break\n",
    "    if debug:\n",
    "        print(name)\n",
    "        print(url)\n",
    "\n",
    "print('completed processing, saving files')\n",
    "pickle.dump(preprocessed_images, open(pre_processed_filename + '.pkl', 'wb'))\n",
    "pickle.dump(targets,open(targets_filename + '.pkl','wb'))\n",
    "pickle.dump(failed_idx, open(failed_filename + '.pkl','wb'))\n",
    "pickle.dump(i,open(total_processed,'wb'))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('elapsed time %0.2f' % ((current_time - start)/60))\n",
    "print('total processed: %d' % i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
